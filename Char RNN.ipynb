{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '(', ')', '*', ',', '-', '.', '0', '3', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '‘', '’', '“', '”', '\\ufeff']\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "filename='alice.txt'\n",
    "f=open(filename,'r',encoding='utf-8')\n",
    "file=f.read()\n",
    "file=file.lower()\n",
    "all_characters=sorted(list(set(file)))\n",
    "print(all_characters)\n",
    "n_characters=len(all_characters)\n",
    "print(n_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144410\n"
     ]
    }
   ],
   "source": [
    "file_len=len(file)\n",
    "print(file_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length=200\n",
    "# chunk_len=100\n",
    "\n",
    "# def random_chunk():\n",
    "#     start=np.random.randint(0,file_len-chunk_len)\n",
    "#     end=start+chunk_len+1\n",
    "#     return file[start:end]\n",
    "\n",
    "# print(random_chunk())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size,output_size,n_layers=1):\n",
    "        super(RNN,self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.output_size=output_size\n",
    "        self.n_layers=n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self,input,hidden):\n",
    "        batch_size=input.size(0)\n",
    "        input=self.encoder(input)\n",
    "        output,hidden=self.gru(input.view(1,batch_size,-1),hidden)\n",
    "        output = self.decoder(output.view(batch_size, -1))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        return torch.zeros(self.n_layers,batch_size,self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_tensor(string):\n",
    "    tensor=torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c]=all_characters.index(string[c])\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set(seq_length, batch_size):\n",
    "    \n",
    "    input = torch.LongTensor(batch_size, seq_length)\n",
    "    target = torch.LongTensor(batch_size, seq_length)\n",
    "    for i in range(batch_size):\n",
    "        start_idx = np.random.randint(0, file_len - seq_length)\n",
    "        end_idx = start_idx + seq_length + 1\n",
    "        seq = file[start_idx:end_idx]\n",
    "        input[i] = char_tensor(seq[:-1])\n",
    "        target[i] = char_tensor(seq[1:])\n",
    "    return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden=charnet.init_hidden(1)\n",
    "    prime_input=char_tensor(prime_str).unsqueeze(0)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        output, hidden = charnet(prime_input[:,p], hidden)\n",
    "    inp = prime_input[:,-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = charnet(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char).unsqueeze(0)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = charnet.init_hidden(batch_size)\n",
    "    charnet.zero_grad()\n",
    "    loss=0\n",
    "    for c in range(seq_length):\n",
    "        output, hidden =charnet(inp[:,c], hidden)\n",
    "        loss += criterion(output.view(batch_size,-1),target[:,c])\n",
    "    loss.backward()\n",
    "    charnet_optimizer.step()\n",
    "\n",
    "    return loss.item()/seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 26s (100 3%) 2.3524]\n",
      "wheiat hal.\n",
      "\n",
      "and bely we thoun ipe pitho aseererd ho  andf too in thenle the orout but ther rangk it!  \n",
      "\n",
      "[0m 52s (200 6%) 2.1839]\n",
      "whe tthed tede own caring frot rengroust as wises, hep and bedt the sin or the thates wast bel do or s \n",
      "\n",
      "[1m 17s (300 10%) 1.9515]\n",
      "what os eanted at mep\n",
      "\n",
      "on, and in dea lit, anking\n",
      "y haid ther, thes the curerp walll sure hers\n",
      "‘ight e \n",
      "\n",
      "[1m 43s (400 13%) 1.9660]\n",
      "whe a mare the davoos with alice and oht astoourting pleassing ‘at out in you bitted loond a one, and  \n",
      "\n",
      "[2m 10s (500 16%) 1.7854]\n",
      "whowa. ‘cainge, and that.’ and, shery litter into it,’ said ale she ware up a sid quie mared a lexdele \n",
      "\n",
      "[2m 39s (600 20%) 1.7799]\n",
      "whil of in, the mody to hat alice woutly the dood in’t one, and at the ‘it thee thouger.’\n",
      "\n",
      "‘didner and \n",
      "\n",
      "[3m 8s (700 23%) 1.6821]\n",
      "whid that and aspight, and the might have tras all! and paning, bus underst the\n",
      "the beathing supp said \n",
      "\n",
      "[3m 38s (800 26%) 1.6685]\n",
      "whe of somen’t alice are the prallok! the dead look evertrild of the don’t not this she *   beging to  \n",
      "\n",
      "[4m 8s (900 30%) 1.5680]\n",
      "wh, and felt the cat! theadly, the cany one cat,’ the\n",
      "las she mil, ‘grot at and she mark it all look a \n",
      "\n",
      "[4m 39s (1000 33%) 1.5216]\n",
      "whink offully to gry on you foock to he begelf it!’ at\n",
      "juch cear to stan, them to gane hear, i mock on \n",
      "\n",
      "[5m 12s (1100 36%) 1.5883]\n",
      "whothor litt before? and she said and his here sion,’ said the cat’t in the cried becooth as the sad a \n",
      "\n",
      "[5m 48s (1200 40%) 1.6411]\n",
      "what elf of so howing\n",
      "and it hinute round all.\n",
      "\n",
      "‘that with sturtle readne, sill all thick to little wa \n",
      "\n",
      "[6m 22s (1300 43%) 1.3209]\n",
      "whoun!’\n",
      "\n",
      "‘them hard if i’m thing whith, will with his stanmon. turtle said the\n",
      "dray.\n",
      "\n",
      "have of the crum \n",
      "\n",
      "[6m 54s (1400 46%) 1.5908]\n",
      "whild dreat she in quite and be\n",
      "with that pasay the pad itho, the crating and odling a don’t\n",
      "in at the \n",
      "\n",
      "[7m 28s (1500 50%) 1.3909]\n",
      "whow,’ the mouse it maked to then and she tried doing the had but and the come to by was, and! and soo \n",
      "\n",
      "[8m 0s (1600 53%) 1.4936]\n",
      "whound have of the right had could one perpoingry, was them a long to sing down it green a cound there \n",
      "\n",
      "[8m 31s (1700 56%) 1.5157]\n",
      "where turgetter on her have a more was\n",
      "a glain there manck to him all grees till over said himportanta \n",
      "\n",
      "[9m 6s (1800 60%) 1.5638]\n",
      "whimpoet, said this thare, four found alice it fare.\n",
      "\n",
      "‘the queen in an were fright her say do a sentis \n",
      "\n",
      "[9m 39s (1900 63%) 1.4571]\n",
      "whirst--how alice gain. ‘down, i went been that is!’ the try.’\n",
      "\n",
      "‘if it were upon the mouse whened more \n",
      "\n",
      "[10m 12s (2000 66%) 1.5585]\n",
      "whing\n",
      "up ace of for firs hery the king, ‘no gring to sboker.\n",
      " ‘with as must it would cater’ll her firs \n",
      "\n",
      "[10m 43s (2100 70%) 1.4430]\n",
      "where was good a trees in\n",
      "the did and hemouse tone. ‘will but on what if it glose at her morgright aft \n",
      "\n",
      "[11m 13s (2200 73%) 1.5622]\n",
      "whise ‘not into heardly, and she ganned at the had some this to leakrouse did nobent! and more seen’t  \n",
      "\n",
      "[11m 44s (2300 76%) 1.4046]\n",
      "when all the\n",
      "why very\n",
      "looking are shoul,’ said the say at the queens. i whiter as she caterpillan of s \n",
      "\n",
      "[12m 14s (2400 80%) 1.4436]\n",
      "while way, and they while who have about it gressores, and the\n",
      "were that she with her had elorder what \n",
      "\n",
      "[12m 46s (2500 83%) 1.3900]\n",
      "whole pusiness the gryphon; and she had very looked on the gry, and i looked this feething many--howin \n",
      "\n",
      "[13m 17s (2600 86%) 1.4719]\n",
      "whick coming the more to him doed alice not may sorthing! no over inding. they leakning omence to say\n",
      " \n",
      "\n",
      "[13m 50s (2700 90%) 1.4451]\n",
      "while gryouse in a fill\n",
      "the march her had neatur till very looked some, i see perhook or a regs, the d \n",
      "\n",
      "[14m 21s (2800 93%) 1.5768]\n",
      "where the gomons\n",
      "to herself,\n",
      "‘what tainty to the king, ‘and there. but i next\n",
      "carmable course to proni \n",
      "\n",
      "[14m 54s (2900 96%) 1.5480]\n",
      "while sure shooked my you near the duchess.\n",
      "\n",
      "‘them once come lifes of\n",
      "lown; ‘but at the suthourrenting \n",
      "\n",
      "[15m 26s (3000 100%) 1.4784]\n",
      "while happed to herself, she thought amould not loskner on\n",
      "it it was grand was not a right was a far,  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 3000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "lr = 0.001\n",
    "batch_size=3\n",
    "\n",
    "charnet=RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "charnet_optimizer = torch.optim.Adam(charnet.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set(seq_length,batch_size))       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(evaluate('wh', 100), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
